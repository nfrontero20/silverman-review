---
title: "Review of Silverman (2020): *Multiple-systems analysis for the quantification of modern slavery: classical and Bayesian approaches*"

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Nicole Frontero
  affiliation: Department of Mathematics and Statistics, Amherst College
  

keywords:
- hidden populations
- human trafficking
- Bayesian-threshold
- multiple-systems estimation

abstract: |
  Multiple-systems estimation is an approach that can be useful for quantifying hidden populations.  This study investigates the stability and robustness of various multiple-systems-estimate methods, including frequentist approaches, existing Bayesian approaches, and a Bayesian-threshold approach that is proposed in this paper.  Three data sets in the field of modern slavery, along with a data set on the Kosovo conflict in 1999, are examined using the aforementioned methods.  By detailing the methodology behind various multiple-systems-estimate approaches and applying those methods to the data sets, the paper provides an overview of how such methods can be used to tackle problems of hidden populations like the quantification of modern slavery.

bibliography: bibliography.bib
output:
  rticles::asa_article:
  keep_tex: yes
---

```{r setup, include = FALSE}
library(tidyverse)
library(mosaic)
library(mdsr)
library(knitr)
library(kableExtra)
```

# Introduction

In his paper "Multiple-systems analysis for the quantification of modern slavery: classical and Bayesian approaches," author Bernard W. Silverman quotes Cockayne: "'without good data on where slaves are, how they become slaves and what happens to them, anti-slavery policy will remain guesswork'" [@cockayneUnshacklingDevelopmentWhy2015; @silvermanMultipleSystemsAnalysis2020].  To the average reader, reading about "good data," a modern-day topic, and "anti-slavery policy," which they likely associate with slavery from centuries ago, may confuse them and leave them wondering just what time period Cockayne is referring to.  However, Silverman quickly brings his readers up to speed by providing an overview of the issue of modern slavery, writing that the original motivation for work on this paper was to contribute to the UK's strategy leading to the Modern Slavery Act of 2015.
    
    
Human trafficking and modern slavery can be difficult topics to research for multiple reasons.  Researchers like Silverman are interested in trying to identify just how many individuals may be suffering from human trafficking or modern slavery, but obviously, due to the covert nature of trafficking and slavery, there is no readily available, comprehensive data set.  Instead, there are lists from various sources that contain counts of victims, but every list just represents a proportion of the population of victims.  Because there is no comprehensive data set of all victims, there is no "truth" available that researchers can use to assess the accuracy of their estimates.  Additionally, the data used to research in this domain is extremely sensitive, and as a result, some information is simply not available to the researcher for reasons of confidentiality.  


Silverman's paper is comparative in nature; he provides an overview of pre-existing methodologies for researching modern day slavery, and he also proposes a new methodology: Bayesian-thresholding.  Silverman provides background information on the standard Poisson model that underlies the various modeling approaches, and does the same for frequentist approaches to model selection and pre-existing Bayesian methods.  He then introduces his new approach of Bayesian-thresholding.  When discussing all of the modeling approaches, Silverman applies the approach to the data sets that he has chosen for this paper. 


# Background
## Data sets
There are four data sets that Silverman refers to and analyzes throughout his paper.  The first is a data set that he analyzed in his 2014 paper "Modern slavery: an application of multiple systems estimation" [@silvermanModernSlaveryApplication2014].  The data set contains six lists from different entities in the United Kingdom.  Consequently, Silverman refers to this data set as the UK data throughout this paper.  Another data set that he references includes six lists of identified victims in the Netherlands between 2010-2015 [@vandijkMonitoringTarget162018; @cruyffChallengeCountingVictims2017].  The third data set comes from data collected by eight agencies in the New Orleans-Metairie metropolitan statistical area (Greater New Orleans) and contains 8 lists (one for each agency) [@balesHowManyTrafficked2020a].  The fourth and final data set examined in this paper contains data on the number of victims killed in armed conflict in Kosovo and is a four list data set [@ballKillingsRefugeeFlow2002].

## Poisson model

Silverman reviews the basic log-linear model proposed by @cormackLogLinearModelsCaptureRecapture1989.  He then applies the concepts from the log-linear model to the UK data set.  For the sake of brevity, we will only discuss the basic log-linear model here.  Suppose there are $K$ lists labeled {1, 2,..., $K$}, for each subset $A$ of {1, 2,..., $K$}, let $N_A$ be the possible number of cases that occur on all the lists in $A$ but on no others.  The "dark figure" is the number of cases $N_\emptyset$ that do not appear on any list.  The most basic model stipulates that $N_A$ has, independently, a Poisson distribution with parameter $\lambda_A$, with some structure on the $\lambda_A$.  Under this model, the "dark figure" $N_\emptyset \sim \text{Poiss(}\lambda_\emptyset\text{)}$.  Silverman ultimately writes that a model that can be used to obtain confidence intervals for the total population size is given by log($\lambda_A$)=$\mu$ + $\sum \limits_{i\in A} \alpha_i$ + $\sum \limits_{i, j\in A; i<j} \beta_{ij}$.  To estimate the "dark figure," you can use exp($\mu$).  There are two approaches to model fitting, which will be discussed in the following sections. 

# Frequentist approaches
Silverman provides an overview of how two frequentist approaches can be used to investigate modern slavery.  The first of these approaches involves adding parameters in a stepwise manner, and is detailed below.

## Adding parameters stepwise
A stepwise approach can be taken to fit a multiple-systems estimation model.  This approach involves starting with main effects only and adding two-list interactions $\beta_{ij}$ stepwise.  After each step, the interaction that improves AIC the most is chosen.  Silverman applies this approach to the UK data and provides a table with estimates and confident intervals by using main effects, and then also by using the stepwise procedure.  

## Use an information criterion

There is another approach for estimation within frequentist thinking besides the aforementioned stepwise procedure.  Silverman describes how one can also fit all possible models (considering every subset of the interactions) and use some criterion (based upon BIC) to choose between them.  However, he notes that the number of lists can introduce an excessive computational burden to this procedure.  Silverman applies this procedure to the UK data and purposefully chooses the five-list data (as opposed to the six-list) in order to reduce computational burden.  Silverman compares the population estimates given by the stepwise procedure and information criterion procedures and notes that models chosen using BIC yield much wider estimates than do models chosen using AIC.  In line with his objective to compare the various multiple-systems estimation methods, Silverman highlights that the estimate of the total population can vary widely depending upon which model is chosen.

## Identifiability and existence of estimates

Silverman acknowledges that with these frequentist approaches, difficulties can arise that one must look out for.  He details three such difficulties, the first of which is that there are no finite maximum likelihood estimates of all of the parameters, and instead, that there is an extended maximum likelihood estimate [@fienbergMaximumLikelihoodEstimation2012].  Another difficulty is that the maximum likelihood estimate may not exist.  The final difficulty that could arise is that the parameters to attain the maximum are unidentifiable, despite the fact that the likelihood can be maximized.  By discussing these three possibilities, Silverman adheres to his mission of providing a comparative overview of methods of multiple-systems estimation.  

# Existing Bayesian approaches
There are two Bayesian approaches that are useful specifically for human rights research.  Silverman describes these approaches and their performance on the data sets in his paper.

## Graphical models
The graphical models approach, developed by @madiganBayesianMethodsEstimation1997 uses every decomposable graph model of dependencies between lists and yields the joint posterior probabilities of the models and the total population size.  Silverman applied this method to the five-list versions of the UK, Netherlands, and New Orleans data, the Kosovo data, and the four-list UK data.  The plot of model results for the UK data shows bimodality, and the Netherlands data, with an extended range, also exhibits bimodality.  Silverman also notes that for the New Orleans data, no model is dominant, while for the Kosovo data, one model is very dominant.  By applying the graphical models approach to the data sets, Silverman provides an application of this method that allows the reader to better grasp the graphical models approach.

## Dirichlet process mixtures
Silverman also provides background on a Bayesian latent class method [@manrique-vallierBayesianPopulationSize2016] and applies it to the data sets.  The key takeaway here is how the method works, so we will not go into how this method performed with the data sets, but will instead provide a general overview of the method.  The Bayesian latent class method provides an MCMC estimate of the population size.  Notably, this method has no restrictions on the number of lists, but because the output is a Monte Carlo estimate, it is necessary to check for sufficient burn-in and that the output has sufficient mixing. 

# Bayesian-threshold approach
After taking the time to review the pre-existing methods for multiple-systems estimation and to also apply those methods to the data sets, Silverman proposes his novel method: a Bayesian-threshold approach. 

## Steps
There are two steps to Silverman's Bayesian-threshold approach.  Step 1 requires specifying a prior that doesn't constrain the intercept parameter or the main effects, but does allow for the prior to shrink the interaction parameters towards 0.  Step 2 involves dropping the interactions for which there is no strong evidence that they are not 0.  Interactions are dropped if the ratio of their posterior mean to their posterior standard deviation does not pass threshold $\tau$.  The analysis then repeats.  

## Choosing a threshold
One of two approaches approaches can be used to choose a threshold $\tau$.  These approaches are referred to as a "liberal" approach and  "parsimonious" approach.  The parsimonious approach uses a much larger threshold, ensuring that interaction parameters will be included only if there is very strong evidence that they are not 0.  In contrast, under the liberal approach, evidence that interaction parameters are not 0 does not have to be as strong.

# Results
Silverman applies the Bayesian-threshold approach to all four data sets, just as he did for all of the other multiple-systems estimating approaches.  The results he provides are lengthy and detailed, so instead, we will focus on the main takeaway from the results from this approach: a Bayesian-threshold model with variance 1 and threshold 2 works relatively well for multiple data sets.  However, Silverman makes sure to advise against applying this method blindly to any data set. 

# Conclusion

Estimating the population of victims suffering from modern day slavery conditions is a difficult task for which there does not yet exist an easy solution.  There are various multiple-systems approaches that can be utilized for analyses, such as a log-linear model, frequentist approaches (adding parameters or information criterion), and Bayesian approaches (graphical models and Dirichlet processes).  In this paper, Silverman proposes another approach which he refers to as a Bayesian-threshold.  Silverman applies the three existing approaches and his novel approach to four data sets, which aids him in his comparative review of multiple-systems estimate approaches.  Silverman concludes that the Bayesian approach with a threshold of 2 and a prior variance of 1 for the interaction parameters that he puts forth is an option for a multiple-systems estimate approach, at least for these data sets.  

Silverman does not propose that multiple-systems estimation at large is *the* answer for working to estimate the population of victims suffering from modern slavery.  Instead, his paper reviews the existing literature on multiple-systems estimation and extends the literature as well.  Silverman offers ideas for future research, for example, writing "how can [multiple-systems methodology] be developed to handle concomitant information and segmentation of populations?"  Silverman recognizes that to better estimate the extent of modern day slavery, more research must be done into the methodology of the estimation.  Although Silverman himself does not explicitly note the power that statisticians and their efforts can have on impacting human rights, his paper provides an excellent view into how statistical knowledge and skills can drive progress and can contribute to meaningful change in the world.  

\newpage
